{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "For this assigment you will be implementing an algorithm which is very commonly used in practice: `Random Forest`.  \n",
    "So what is `Random Forest`? To answer this question, let's first learn a couple of new statistical concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Recall `Central Limit Theorem` from your probability and statistics course. It states that if you have a population with mean $\\mu$ and standard deviation $\\sigma$ and take sufficiently large random samples from the population with replacement, then the distribution of the sample means will be approximately normally distributed. This will hold true regardless of whether the source population is normal or skewed, provided the sample size is sufficiently large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretly, we can say that if:\n",
    "\n",
    "$$ X_1, X_2, \\ldots, X_n \\sim P(X) $$\n",
    "\n",
    "where $X_i$ are i.i.d. random samples from any distribution $P(X)$ with mean $\\mu$ and standard deviation $\\sigma$, then we have:\n",
    "$$ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ Var(\\bar{X}) = \\frac{\\sigma^2}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the case of Machine Learning, we are many times interested in decreasing the variance of our model.  \n",
    "- In `Ensemble Methods`, we do so by training multiple different machine learning models such as (Logistic Regression, Decision Trees, Support Vector Machines, etc.) and then combining them in some way to get a final prediction.  \n",
    "- Doing so, we are able to decrease the variance of our model and thus improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula for variance reduction works only if the samples are independent. In practice, this is not the case. There tends to be a certain amount of correlation between the samples. The equation for variance reduction can be modified to account for this correlation.\n",
    "\n",
    "$$ Var(\\bar{X}) = \\rho \\sigma^2 + \\frac{1 - \\rho}{n} \\sigma^2$$\n",
    "\n",
    "where $\\rho$ is the correlation between the samples.\n",
    "\n",
    "Notice that if $\\rho = 0$, then we get the original formula for variance reduction. and if $\\rho = 1$, then there is no variance reduction at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different forms of ensemble methods. Each of those try to decrease this correlation term ($\\rho$) as much as possible.  \n",
    "In this assignment(`Random Forest`), we will be focusing on one specific type called `Bagging`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "For the baseline model, implement a normal `Decision Tree` classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Proprocessing (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "maternal_health_risk = fetch_ucirepo(id=863) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = maternal_health_risk.data.features \n",
    "y = maternal_health_risk.data.targets \n",
    "  \n",
    "# Perform train-test split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Split the train data into train and validation data\n",
    "X_train, X_val, y_train, y_val = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do any preprocessing here - correlation matrices, avg metrics, plots, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Decision Tree class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    # Implement the decision tree class here from the ideas you have learnt in classes\n",
    "    \n",
    "    def __init__(self, max_depth=None):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values (class labels) as integers or strings.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): Input data to predict, where each element is an array of features.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: Predicted outputs for each input in X.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively grows a decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "        y (numpy.ndarray): Target vector of shape (n_samples,).\n",
    "        depth (int): Current depth of the tree. Default is 0.\n",
    "\n",
    "        Returns:\n",
    "        Node: A node of the decision tree.\n",
    "\n",
    "        The function stops growing the tree if any of the following conditions are met:\n",
    "        - The maximum depth specified by self.max_depth is reached.\n",
    "        - All the labels are the same.\n",
    "        - The number of samples is less than 2.\n",
    "\n",
    "        The function selects the best feature and threshold to split the data based on Gini impurity.\n",
    "        It then recursively grows the left and right subtrees.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        \"\"\"\n",
    "        Determine the best feature and threshold for splitting the data based on the Gini impurity.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): The feature matrix of shape (n_samples, n_features).\n",
    "        y (numpy.ndarray): The target vector of shape (n_samples,).\n",
    "        feat_idxs (list): List of feature indices to consider for the split.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing the index of the best feature to split on and the best threshold value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _gini(self, y, X_column, split_thresh):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity for a split.\n",
    "\n",
    "        Parameters:\n",
    "        y (array-like): The target values.\n",
    "        X_column (array-like): The feature values for the column being split.\n",
    "        split_thresh (float): The threshold value to split the feature column.\n",
    "\n",
    "        Returns:\n",
    "        float: The Gini impurity of the split.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity for a set of labels.\n",
    "\n",
    "        Gini impurity is a measure of how often a randomly chosen element from the set\n",
    "        would be incorrectly labeled if it was randomly labeled according to the distribution\n",
    "        of labels in the set.\n",
    "\n",
    "        Parameters:\n",
    "        y (array-like): Array of labels.\n",
    "\n",
    "        Returns:\n",
    "        float: Gini impurity of the set of labels.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        \"\"\"\n",
    "        Splits the indices of the input column into two groups based on the split threshold.\n",
    "\n",
    "        Parameters:\n",
    "        X_column (numpy.ndarray): The input column to split.\n",
    "        split_thresh (float): The threshold value to split the column.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing two numpy arrays:\n",
    "            - left_idxs (numpy.ndarray): Indices where the column values are less than or equal to the split threshold.\n",
    "            - right_idxs (numpy.ndarray): Indices where the column values are greater than the split threshold.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        \"\"\"\n",
    "        Finds the most common label in the array of labels.\n",
    "\n",
    "        Parameters:\n",
    "        y (array-like): Array of labels.\n",
    "\n",
    "        Returns:\n",
    "        object: The most common label in the array.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the training data and report the accuracy and [F1 score](https://en.wikipedia.org/wiki/F-score) on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging ([Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) Aggregation) is a method of constructing an ensemble of classifiers by training each classifier on a random subset (bootstrap sample) of the training set. Concretly,\n",
    "\n",
    "- Given a training set $X = x_1, x_2, \\ldots, x_n$ with labels $Y = y_1, y_2, \\ldots, y_n$.\n",
    "- We create $k$ bootstrap samples $X_1, X_2, \\ldots, X_k$ by sampling $r$ examples from $X$ uniformly at random with replacement.\n",
    "- We train $k$ different classifiers $h_1, h_2, \\ldots, h_k$ on the bootstrap samples $X_1, X_2, \\ldots, X_k$.\n",
    "- We combine the predictions of each classifier using majority voting.\n",
    "\n",
    "In doing so, we are able to decrease the correlation between the classifiers and thus decrease the variance of our model.  \n",
    "Notice that if we use the same training set to train each classifier, then the correlation between the classifiers will be very high ($\\rho \\approx 1$) and thus there will be almost no variance reduction at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function which creates bootstrap samples from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(df: pd.DataFrame, n_samples: int, sample_fraction: float, target: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"Generate bootstrap samples using the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to generate bootstrap samples from.\n",
    "        n_samples (int): The number of bootstrap samples to generate.\n",
    "        sample_fraction (float): The fraction of the dataframe to use for each sample with replacement. (between 0 and 1)\n",
    "        target (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of bootstrap samples.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take 'n' bootstrap samples from the training data and train a `Decision Tree` classifier on each of them. Make sure you use the `DecisionTree` class you implemented above to train the models and store the trained trees in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "sample_fraction = 0.8\n",
    "target = None # TODO: Set the target column\n",
    "trees = []\n",
    "\n",
    "bootstrap_samples = get_bootstrap_samples(df_train, n_trees, sample_fraction, target)\n",
    "\n",
    "# Write your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained trees to make predictions on the validation data and report the accuracy and [F1 Score](https://en.wikipedia.org/wiki/F-score) on the validation data. To make the prediction follow the following steps:\n",
    "- For each data point in the validation set, make predictions using each of the trained trees. (you will have 'n_trees' predictions for each data point)\n",
    "- Combine the predictions of each classifier using majority voting. \n",
    "- eg. If you have 5 trees and 3 of them predict __class 1__ and 2 of them predict __class 2__, then the final prediction will be __class 1__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random Forest` is a special type of bagging algorithm where we not only train each classifier on a bootstrap sample of the training data but also a random subset of the features. This is done to further decrease the correlation between the classifiers. Concretly,\n",
    "\n",
    "- Given a training set $X = x_1, x_2, \\ldots, x_n$ with labels $Y = y_1, y_2, \\ldots, y_n$.\n",
    "- We create $k$ bootstrap samples $X_1, X_2, \\ldots, X_k$ by sampling $r$ examples from $X$ uniformly at random with replacement.\n",
    "- Each time we create a bootstrap sample, we also select a random subset of the features $F_i$ to train the classifier on.\n",
    "- We train $k$ different classifiers $h_1, h_2, \\ldots, h_k$ on the bootstrap samples $X_1, X_2, \\ldots, X_k$.\n",
    "- We combine the predictions of each classifier using majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this new version of bootstrap sampling below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_bootstrap_samples(df: pd.DataFrame, n_samples: int, sample_fraction: float, feature_fraction: float, target: str) -> Tuple[List[pd.DataFrame], List[str]]:\n",
    "    \"\"\"Generate bootstrap samples using the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to generate bootstrap samples from.\n",
    "        n_samples (int): The number of bootstrap samples to generate.\n",
    "        sample_fraction (float): The fraction of the dataframe to use for each sample with replacement. (between 0 and 1)\n",
    "        feature_fraction (float): The fraction of features to use for each sample. (between 0 and 1)\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of bootstrap samples.\n",
    "        List[str]: A list of features used for each sample.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take 'n' bootstrap samples from the training data and train a `Decision Tree` classifier on each of them. Make sure you use the `DecisionTree` class you implemented above to train the models and store the trained trees in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "sample_fraction = 0.8\n",
    "feature_fraction = 0.7\n",
    "target = None # TODO: Set the target column\n",
    "trees = []\n",
    "\n",
    "bootstrap_samples, sample_features = get_random_forest_bootstrap_samples(df_train, n_trees, sample_fraction, feature_fraction, target)\n",
    "\n",
    "# Write your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained trees to make predictions on the validation data and report the accuracy and [F1 Score](https://en.wikipedia.org/wiki/F-score) on the validation data. Use the sample prediction method as in bagging. Try modifying the different parameters to get the best results on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to try out once you have a working implementation:\n",
    "- Try limiting the depth of the trees\n",
    "- Try using different voting methods (weighted voting, etc.)\n",
    "- Try using different methods to select the best split (information gain, gini index, etc.)\n",
    "\n",
    "Try these ideas **ONLY AFTER** you have a working implementation. These ideas are not guaranteed to improve your score but are worth trying out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your ensemble model and use it to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check for our implementation\n",
    "Here, we compute the results with scikit-learn's implementation of decision trees/random forest. Do you see a performance difference? What changes can you make?\n",
    "\n",
    "For reference: this is scikit-learn's implementation: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn's RandomForestClassifier to train and evaluate the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
